@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})

@misc{Authors14,
 author = {FirstName LastName},
 title = {The frobnicatable foo filter},
 note = {Face and Gesture submission ID 324. Supplied as supplemental material {\tt fg324.pdf}},
 year = 2014
}

@misc{Authors14b,
 author = {FirstName LastName},
 title = {Frobnication tutorial},
 note = {Supplied as supplemental material {\tt tr.pdf}},
 year = 2014
}

@article{Alpher02,
author = {FirstName Alpher},
title = {Frobnication},
journal = PAMI,
volume = 12,
number = 1,
pages = {234--778},
year = 2002
}

@article{Alpher03,
author = {FirstName Alpher and  FirstName Fotheringham-Smythe},
title = {Frobnication revisited},
journal = {Journal of Foo},
volume = 13,
number = 1,
pages = {234--778},
year = 2003
}

@article{Alpher04,
author = {FirstName Alpher and FirstName Fotheringham-Smythe and FirstName Gamow},
title = {Can a machine frobnicate?},
journal = {Journal of Foo},
volume = 14,
number = 1,
pages = {234--778},
year = 2004
}

@inproceedings{Alpher05,
author = {FirstName Alpher and FirstName Gamow},
title = {Can a computer frobnicate?},
booktitle = ICCV,
pages = {234--778},
year = 2005
}
@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})



@misc{Authors14,
 author = {FirstName LastName},
 title = {The frobnicatable foo filter},
 note = {Face and Gesture submission ID 324. Supplied as supplemental material {\tt fg324.pdf}},
 year = 2014
}

@misc{Authors14b,
 author = {FirstName LastName},
 title = {Frobnication tutorial},
 note = {Supplied as supplemental material {\tt tr.pdf}},
 year = 2014
}

@article{Alpher02,
author = {FirstName Alpher},
title = {Frobnication},
journal = PAMI,
volume = 12,
number = 1,
pages = {234--778},
year = 2002
}

@article{Alpher03,
author = {FirstName Alpher and  FirstName Fotheringham-Smythe},
title = {Frobnication revisited},
journal = {Journal of Foo},
volume = 13,
number = 1,
pages = {234--778},
year = 2003
}

@article{Alpher04,
author = {FirstName Alpher and FirstName Fotheringham-Smythe and FirstName Gamow},
title = {Can a machine frobnicate?},
journal = {Journal of Foo},
volume = 14,
number = 1,
pages = {234--778},
year = 2004
}

@inproceedings{Alpher05,
author = {FirstName Alpher and FirstName Gamow},
title = {Can a computer frobnicate?},
booktitle = ICCV,
pages = {234--778},
year = 2005
}

@article{liu2023visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={34892--34916},
  year={2023}
}


@article{zhu2023minigpt,
  title={Minigpt-4: Enhancing vision-language understanding with advanced large language models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2304.10592},
  year={2023}
}

@misc{dai2023instructblip,
      title={InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning}, 
      author={Wenliang Dai and Junnan Li and Dongxu Li and Anthony Meng Huat Tiong and Junqi Zhao and Weisheng Wang and Boyang Li and Pascale Fung and Steven Hoi},
      year={2023},
      eprint={2305.06500},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2305.06500}, 
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{devlin2019bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers)},
  pages={4171--4186},
  year={2019}
}

@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}

@article{bai2023qwen, title={Qwen technical report}, author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others}, journal={arXiv preprint arXiv:2309.16609}, year={2023} }


@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={San Francisco, CA, USA}
}



@article{li2023videochat,
  title={Videochat: Chat-centric video understanding},
  author={Li, KunChang and He, Yinan and Wang, Yi and Li, Yizhuo and Wang, Wenhai and Luo, Ping and Wang, Yali and Wang, Limin and Qiao, Yu},
  journal={arXiv preprint arXiv:2305.06355},
  year={2023}
}

@article{zhang2023video,
  title={Video-llama: An instruction-tuned audio-visual language model for video understanding},
  author={Zhang, Hang and Li, Xin and Bing, Lidong},
  journal={arXiv preprint arXiv:2306.02858},
  year={2023}
}

@article{cheng2024videollama,
  title={Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms},
  author={Cheng, Zesen and Leng, Sicong and Zhang, Hang and Xin, Yifei and Li, Xin and Chen, Guanzheng and Zhu, Yongxin and Zhang, Wenqi and Luo, Ziyang and Zhao, Deli and others},
  journal={arXiv preprint arXiv:2406.07476},
  year={2024}
}

@article{maaz2023video,
  title={Video-chatgpt: Towards detailed video understanding via large vision and language models},
  author={Maaz, Muhammad and Rasheed, Hanoona and Khan, Salman and Khan, Fahad Shahbaz},
  journal={arXiv preprint arXiv:2306.05424},
  year={2023}
}

@article{chen2024sharegpt4video,
  title={ShareGPT4Video: Improving Video Understanding and Generation with Better Captions},
  author={Chen, Lin and Wei, Xilin and Li, Jinsong and Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Chen, Zehui and Duan, Haodong and Lin, Bin and Tang, Zhenyu and others},
  journal={arXiv preprint arXiv:2406.04325},
  year={2024}
}


@article{chen2024we,
  title={Are We on the Right Way for Evaluating Large Vision-Language Models?},
  author={Chen, Lin and Li, Jinsong and Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Chen, Zehui and Duan, Haodong and Wang, Jiaqi and Qiao, Yu and Lin, Dahua and others},
  journal={arXiv preprint arXiv:2403.20330},
  year={2024}
}


@article{gong2023multimodal,
  title={Multimodal-gpt: A vision and language model for dialogue with humans},
  author={Gong, Tao and Lyu, Chengqi and Zhang, Shilong and Wang, Yudong and Zheng, Miao and Zhao, Qian and Liu, Kuikun and Zhang, Wenwei and Luo, Ping and Chen, Kai},
  journal={arXiv preprint arXiv:2305.04790},
  year={2023}
}


@article{apollo,
    title={Apollo: An Exploration of Video Understanding in Large Multimodal Models},
    author={Orr Zohar, Xiaohan Wang, Yann Dubois, Nikhil Mehta, Tong Xiao, Philippe Hansen-Estruch, Licheng Yu, Xiaofang Wang, Felix Juefei-Xu, Ning Zhang, Serena Yeung-Levy, and Xide Xia},
    journal={arXiv preprint arXiv:2412.10360},
    year={2024}
}

@misc{abouelenin2025phi4minitechnicalreportcompact,
      title={Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs}, 
      author={Abdelrahman Abouelenin and Atabak Ashfaq and Adam Atkinson and Hany Awadalla and Nguyen Bach and Jianmin Bao and Alon Benhaim and Martin Cai and Vishrav Chaudhary and Congcong Chen and Dong Chen and Dongdong Chen and Junkun Chen and Weizhu Chen and Yen-Chun Chen and Yi-ling Chen and Qi Dai and Xiyang Dai and Ruchao Fan and Mei Gao and Min Gao and Amit Garg and Abhishek Goswami and Junheng Hao and Amr Hendy and Yuxuan Hu and Xin Jin and Mahmoud Khademi and Dongwoo Kim and Young Jin Kim and Gina Lee and Jinyu Li and Yunsheng Li and Chen Liang and Xihui Lin and Zeqi Lin and Mengchen Liu and Yang Liu and Gilsinia Lopez and Chong Luo and Piyush Madan and Vadim Mazalov and Ali Mousavi and Anh Nguyen and Jing Pan and Daniel Perez-Becker and Jacob Platin and Thomas Portet and Kai Qiu and Bo Ren and Liliang Ren and Sambuddha Roy and Ning Shang and Yelong Shen and Saksham Singhal and Subhojit Som and Xia Song and Tetyana Sych and Praneetha Vaddamanu and Shuohang Wang and Yiming Wang and Zhenghao Wang and Haibin Wu and Haoran Xu and Weijian Xu and Yifan Yang and Ziyi Yang and Donghan Yu and Ishmam Zabir and Jianwen Zhang and Li Lyna Zhang and Yunan Zhang and Xiren Zhou},
      year={2025},
      eprint={2503.01743},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2503.01743}, 
}

@article{hurst2024gpt,
  title={Gpt-4o system card},
  author={Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}

@article{Tang2024SparkleMB,
title={Sparkle: Mastering Basic Spatial Capabilities in Vision Language Models Elicits Generalization to Composite Spatial Reasoning},
author={Yihong Tang and Ao Qu and Zhaokai Wang and Dingyi Zhuang and Zhaofeng Wu and Wei Ma and Shenhao Wang and Yunhan Zheng and Zhan Zhao and Jinhua Zhao},
journal={ArXiv},
year={2024},
volume={abs/2410.16162},
url={https://api.semanticscholar.org/CorpusID:273507080}
}

@article{yang2023setofmark,
    title={Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V}, 
    author={Jianwei Yang and Hao Zhang and Feng Li and Xueyan Zou and Chunyuan Li and Jianfeng Gao},
    journal={arXiv preprint arXiv:2310.11441},
    year={2023},
}


@article{yang2024think,
    title={{Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces}},
    author={Yang, Jihan and Yang, Shusheng and Gupta, Anjali W. and Han, Rilyn and Fei-Fei, Li and Xie, Saining},
    year={2024},
    journal={arXiv preprint arXiv:2412.14171},
}
@article{Freyd1984Representational,
  author = {Freyd, Jennifer J. and Finke, Ronald A.},
  title = {Representational momentum},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {10},
  number = {1},
  pages = {126--132},
  year = {1984},
  doi = {10.1037/0278-7393.10.1.126}
}

@misc{zhang2024videoinstructiontuningsynthetic,
  title={Video Instruction Tuning With Synthetic Data},
  author={Yuanhan Zhang and Jinming Wu and Wei Li and Bo Li and Zejun Ma and Ziwei Liu and Chunyuan Li},
  year={2024},
  eprint={2410.02713},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2410.02713},
}

@inproceedings{Lu2019ViLBERTPT,
  title={ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks},
  author={Jiasen Lu and Dhruv Batra and Devi Parikh and Stefan Lee},
  booktitle={Neural Information Processing Systems},
  year={2019}
}

@article{liu2023world,
  title={World Model on Million-Length Video and Language with RingAttention},
  author={Liu, Hao and Yan, Wilson and Zaharia, Matei and Abbeel, Pieter},
  journal={arXiv preprint},
  year={2024}
}


@article{wang2024compositional,
  title={Compositional 4D Dynamic Scenes Understanding with Physics Priors for Video Question Answering},
  author={Wang, Xingrui and Ma, Wufei and Wang, Angtian and Chen, Shuo and Kortylewski, Adam and Yuille, Alan},
  journal={arXiv preprint arXiv:2406.00622},
  year={2024}
}

@misc{cui2025comprehensive,
author= {Erfei Cui and Yinan He and Zheng Ma and Zhe Chen and Hao Tian and Weiyun Wang and Kunchang Li and Yi Wang and Wenhai Wang and Xizhou Zhu and Lewei Lu and Tong Lu and Yali Wang and Limin Wang and Yu Qiao and Jifeng Dai},
title        = {ShareGPT-4o: Comprehensive Multimodal Annotations With GPT-4o},year         = {2024},
url= {https://sharegpt4o.github.io/}, 
}

@article{Spelke2007Core,
  author = {Spelke, Elizabeth S. and Kinzler, Katherine D.},
  title = {Core knowledge},
  journal = {Developmental Science},
  volume = {10},
  number = {1},
  pages = {89--96},
  year = {2007},
  doi = {10.1111/j.1467-7687.2007.00569.x},
  publisher = {Blackwell Publishing Ltd}
}


@InProceedings{Chen_2022_CVPR,
  author = {Chen, Jun and Guo, Han and Yi, Kai and Li, Boyang and Elhoseiny, Mohamed},
  title = {VisualGPT: Data-Efficient Adaptation of Pretrained Language Models for Image Captioning},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month = {June},
  year = {2022},
  pages = {18030-18040}
}


@article{LeCun1989Backpropagation,
  author = {LeCun, Yann and Boser, Bernhard and Denker, John S. and Henderson, Donnie and Howard, Richard E. and Hubbard, Wayne and Jackel, Lawrence D.},
  title = {Backpropagation Applied to Handwritten Zip Code Recognition},
  journal = {Neural Computation},
  volume = {1},
  number = {4},
  pages = {541--551},
  year = {1989},
  doi = {10.1162/neco.1989.1.4.541},
  publisher = {MIT Press}
}

@inproceedings{Dosovitskiy2021An,
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  booktitle = {International Conference on Learning Representations},
  year = {2021}
}

@inproceedings{Radford2021Learning,
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  title = {Learning Transferable Visual Models From Natural Language Supervision},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  series = {PMLR},
  volume = {139},
  pages = {8748--8763},
  year = {2021}
}


@article{Burgess2006Spatial,
  author = {Burgess, Neil},
  title = {Spatial memory: how egocentric and allocentric combine},
  journal = {Trends in Cognitive Sciences},
  volume = {10},
  number = {12},
  pages = {551--557},
  year = {2006},
  doi = {10.1016/j.tics.2006.10.005},
  publisher = {Elsevier}
}

---
Answer from Perplexity: pplx.ai/share
@article{Burgess2006Spatial,
  author = {Burgess, Neil},
  title = {Spatial memory: how egocentric and allocentric combine},
  journal = {Trends in Cognitive Sciences},
  volume = {10},
  number = {12},
  pages = {551--557},
  year = {2006},
  doi = {10.1016/j.tics.2006.10.005},
  publisher = {Elsevier},
  url = {https://www.sciencedirect.com}
}


@article{Johansson1973Visual,
  author = {Johansson, Gunnar},
  title = {Visual perception of biological motion and a model for its analysis},
  journal = {Perception \& Psychophysics},
  volume = {14},
  number = {2},
  pages = {201--211},
  year = {1973},
  doi = {10.3758/BF03212378},
  publisher = {Springer}
}


@article{Marr1981Directional,
  author = {Marr, D. and Ullman, S.},
  title = {Directional selectivity and its use in early visual processing},
  journal = {Proceedings of the Royal Society of London. Series B, Biological Sciences},
  volume = {211},
  number = {1183},
  pages = {151--180},
  year = {1981},
  doi = {10.1098/rspb.1981.0001},
  publisher = {The Royal Society}
}

@misc{claude,
  author = {Anthropic},
  title = {Introducing the next generation of {Claude}},
  howpublished = {\url{https://www.anthropic.com/news/claude-3-family}},
year      = {2024},
  note = {Accessed: 2024-07-29}
}

@article{Leslie1984Spatiotemporal,
  author = {Leslie, Alan M.},
  title = {Spatiotemporal continuity and the perception of causality in infants},
  journal = {Perception},
  volume = {13},
  number = {3},
  pages = {287--305},
  year = {1984},
  doi = {10.1068/p130287}
}


@article{DeFreitas2016Tracking,
  author = {De Freitas, Julian and Myers, Nicholas E. and Nobre, Anna C.},
  title = {Tracking the changing feature of a moving object},
  journal = {Journal of Vision},
  volume = {16},
  number = {3},
  pages = {22},
  year = {2016},
  doi = {10.1167/16.3.22},
  publisher = {ARVO}
}


@article{Meta2025VideoJAM,
  title={Meta's VideoJAM: Innovation in AI Video Motion},
  author={Meta AI Research},
  journal={Meta AI Research Technical Report},
  year={2025},
  month={02}
}

@article{PhysGen2024,
  title={PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation},
  author={Liu, Shaowei and others},
  journal={arXiv preprint},
  year={2024},
  month={10}
}


@article{DirectorLLM2024,
  title={DirectorLLM for Human-Centric Video Generation},
  author={Research Team},
  journal={arXiv preprint},
  year={2024},
  month={12}
}





@article{Luo2025EnhanceAVideo,
  title={Enhance-A-Video: Better Generated Video for Free},
  author={Luo, Yang and others},
  journal={arXiv preprint arXiv:2502.07508},
  year={2025},
  month={02}
}

@article{liu2023fetv,
  title   = {FETV: A Benchmark for Fine-Grained Evaluation of Open-Domain Text-to-Video Generation},
  author  = {Yuanxin Liu and Lei Li and Shuhuai Ren and Rundong Gao and Shicheng Li and Sishuo Chen and Xu Sun and Lu Hou},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2311.01813}
}

@InProceedings{Zhang_2024_CVPR,
  author = {Zhang, Yichi and Dong, Yinpeng and Zhang, Siyuan and Min, Tianzan and Su, Hang and Zhu, Jun},
  title = {Exploring the Transferability of Visual Prompting for Multimodal Large Language Models},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month = {June},
  year = {2024},
  pages = {26562-26572}
}

@article{Wan2024ContrastiveRG,
  title={Contrastive Region Guidance: Improving Grounding in Vision-Language Models without Training},
  author={David Wan and others},
  journal={arXiv preprint arXiv:2403.02325},
  year={2024}
}

@misc{2502.02492,
        Author = {Hila Chefer and Uriel Singer and Amit Zohar and Yuval Kirstain and Adam Polyak and Yaniv Taigman and Lior Wolf and Shelly Sheynin},
        Title = {VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models},
        Year = {2025},
        Eprint = {arXiv:2502.02492},
        }

@inproceedings{
ramakrishnan2025does,
title={Does Spatial Cognition Emerge in Frontier Models?},
author={Santhosh Kumar Ramakrishnan and Erik Wijmans and Philipp Kraehenbuehl and Vladlen Koltun},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=WK6K1FMEQ1}
}

@article{li2024llava,
  title={Llava-onevision: Easy visual task transfer},
  author={Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Zhang, Peiyuan and Li, Yanwei and Liu, Ziwei and others},
  journal={arXiv preprint arXiv:2408.03326},
  year={2024}
}


@article{team2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}


@article{wang2024qwen2,
  title={Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}


@article{suglia2024alanavlm,
  title={Alanavlm: A multimodal embodied ai foundation model for egocentric video understanding},
  author={Suglia, Alessandro and Greco, Claudio and Baker, Katie and Part, Jose L and Papaioannou, Ioannis and Eshghi, Arash and Konstas, Ioannis and Lemon, Oliver},
  journal={arXiv preprint arXiv:2406.13807},
  year={2024}
}

@article{driess2023palm,
  title={Palm-e: An embodied multimodal language model},
  author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and Huang, Wenlong and others},
  year={2023}
}


@article{kim2024openvla,
  title={Openvla: An open-source vision-language-action model},
  author={Kim, Moo Jin and Pertsch, Karl and Karamcheti, Siddharth and Xiao, Ted and Balakrishna, Ashwin and Nair, Suraj and Rafailov, Rafael and Foster, Ethan and Lam, Grace and Sanketi, Pannag and others},
  journal={arXiv preprint arXiv:2406.09246},
  year={2024}
}


@article{liu2024world,
  title={World model on million-length video and language with blockwise ringattention},
  author={Liu, Hao and Yan, Wilson and Zaharia, Matei and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2402.08268},
  year={2024}
}


@article{wang2024vlm,
  title={Vlm see, robot do: Human demo video to robot action plan via vision language model},
  author={Wang, Beichen and Zhang, Juexiao and Dong, Shuwen and Fang, Irving and Feng, Chen},
  journal={arXiv preprint arXiv:2410.08792},
  year={2024}
}


@article{patel2025real,
  title={A Real-to-Sim-to-Real Approach to Robotic Manipulation with VLM-Generated Iterative Keypoint Rewards},
  author={Patel, Shivansh and Yin, Xinchen and Huang, Wenlong and Garg, Shubham and Nayyeri, Hooshang and Fei-Fei, Li and Lazebnik, Svetlana and Li, Yunzhu},
  journal={arXiv preprint arXiv:2502.08643},
  year={2025}
}


@article{zhang2024combo,
  title={COMBO: compositional world models for embodied multi-agent cooperation},
  author={Zhang, Hongxin and Wang, Zeyuan and Lyu, Qiushi and Zhang, Zheyuan and Chen, Sunli and Shu, Tianmin and Du, Yilun and Gan, Chuang},
  journal={arXiv preprint arXiv:2404.10775},
  year={2024}
}

@article{agarwal2025cosmos,
  title={Cosmos world foundation model platform for physical ai},
  author={Agarwal, Niket and Ali, Arslan and Bala, Maciej and Balaji, Yogesh and Barker, Erik and Cai, Tiffany and Chattopadhyay, Prithvijit and Chen, Yongxin and Cui, Yin and Ding, Yifan and others},
  journal={arXiv preprint arXiv:2501.03575},
  year={2025}
}



@article{khattak2024good,
  title={How Good is my Video LMM? Complex Video Reasoning and Robustness Evaluation Suite for Video-LMMs},
  author={Khattak, Muhammad Uzair and Naeem, Muhammad Ferjad and Hassan, Jameel and Naseer, Muzammal and Tombari, Federico and Khan, Fahad Shahbaz and Khan, Salman},
  journal={arXiv preprint arXiv:2405.03690},
  year={2024}
}

@article{li2024videoeval,
  title={Videoeval: Comprehensive benchmark suite for low-cost evaluation of video foundation model},
  author={Li, Xinhao and Huang, Zhenpeng and Wang, Jing and Li, Kunchang and Wang, Limin},
  journal={arXiv preprint arXiv:2407.06491},
  year={2024}
}


@article{fu2024video,
  title={Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis},
  author={Fu, Chaoyou and Dai, Yuhan and Luo, Yongdong and Li, Lei and Ren, Shuhuai and Zhang, Renrui and Wang, Zihan and Zhou, Chenyu and Shen, Yunhang and Zhang, Mengdan and others},
  journal={arXiv preprint arXiv:2405.21075},
  year={2024}
}

@inproceedings{li2024mvbench,
  title={Mvbench: A comprehensive multi-modal video understanding benchmark},
  author={Li, Kunchang and Wang, Yali and He, Yinan and Li, Yizhuo and Wang, Yi and Liu, Yi and Wang, Zun and Xu, Jilan and Chen, Guo and Luo, Ping and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={22195--22206},
  year={2024}
}

@article{ning2023video, title={Video-bench: A comprehensive benchmark and toolkit for evaluating video-based large language models}, author={Ning, Munan and Zhu, Bin and Xie, Yujia and Lin, Bin and Cui, Jiaxi and Yuan, Lu and Chen, Dongdong and Yuan, Li}, journal={arXiv preprint arXiv:2311.16103}, year={2023} }


@article{yang2024thinking,
  title={Thinking in space: How multimodal large language models see, remember, and recall spaces},
  author={Yang, Jihan and Yang, Shusheng and Gupta, Anjali W and Han, Rilyn and Fei-Fei, Li and Xie, Saining},
  journal={arXiv preprint arXiv:2412.14171},
  year={2024}
}


@inproceedings{li2024seed,
  title={Seed-bench: Benchmarking multimodal large language models},
  author={Li, Bohao and Ge, Yuying and Ge, Yixiao and Wang, Guangzhi and Wang, Rui and Zhang, Ruimao and Shan, Ying},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13299--13308},
  year={2024}
}


@inproceedings{liu2024mmbench,
  title={Mmbench: Is your multi-modal model an all-around player?},
  author={Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others},
  booktitle={European conference on computer vision},
  pages={216--233},
  year={2024},
  organization={Springer}
}

@inproceedings{yue2024mmmu,
  title={Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi},
  author={Yue, Xiang and Ni, Yuansheng and Zhang, Kai and Zheng, Tianyu and Liu, Ruoqi and Zhang, Ge and Stevens, Samuel and Jiang, Dongfu and Ren, Weiming and Sun, Yuxuan and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9556--9567},
  year={2024}
}

@article{he2024mmworld,
  title={Mmworld: Towards multi-discipline multi-faceted world model evaluation in videos},
  author={He, Xuehai and Feng, Weixi and Zheng, Kaizhi and Lu, Yujie and Zhu, Wanrong and Li, Jiachen and Fan, Yue and Wang, Jianfeng and Li, Linjie and Yang, Zhengyuan and others},
  journal={arXiv preprint arXiv:2406.08407},
  year={2024}
}

@article{davis2017,
  title={The 2017 davis challenge on video object segmentation},
  author={Pont-Tuset, Jordi and Perazzi, Federico and Caelles, Sergi and Arbel{\'a}ez, Pablo and Sorkine-Hornung, Alex and Van Gool, Luc},
  journal={arXiv preprint arXiv:1704.00675},
  year={2017}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}
@article{zhao2025mmvu,
  title={MMVU: Measuring Expert-Level Multi-Discipline Video Understanding},
  author={Zhao, Yilun and Xie, Lujing and Zhang, Haowei and Gan, Guo and Long, Yitao and Hu, Zhiyuan and Hu, Tongyan and Chen, Weiyuan and Li, Chuhan and Song, Junyang and others},
  journal={arXiv preprint arXiv:2501.12380},
  year={2025}
}

@article{wu2024videollm,
  title={Videollm-mod: Efficient video-language streaming with mixture-of-depths vision computation},
  author={Wu, Shiwei and Chen, Joya and Lin, Kevin Qinghong and Wang, Qimeng and Gao, Yan and Xu, Qianli and Xu, Tong and Hu, Yao and Chen, Enhong and Shou, Mike Zheng},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={109922--109947},
  year={2024}
}


@article{yuan2024videorefer,
  title={Videorefer suite: Advancing spatial-temporal object understanding with video llm},
  author={Yuan, Yuqian and Zhang, Hang and Li, Wentong and Cheng, Zesen and Zhang, Boqiang and Li, Long and Li, Xin and Zhao, Deli and Zhang, Wenqiao and Zhuang, Yueting and others},
  journal={arXiv preprint arXiv:2501.00599},
  year={2024}
}


@article{he2024mojito,
  title   = {Mojito: Motion Trajectory and Intensity Control for Video Generation},
  author  = {Xuehai He and Shuohang Wang and Jianwei Yang and Xiaoxia Wu and Yiping Wang and Kuan Wang and Zheng Zhan and Olatunji Ruwase and Yelong Shen and Xin Eric Wang},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2412.08948}
}

@article{wang2024grounded,
  title={Grounded-videollm: Sharpening fine-grained temporal grounding in video large language models},
  author={Wang, Haibo and Xu, Zhiyang and Cheng, Yu and Diao, Shizhe and Zhou, Yufan and Cao, Yixin and Wang, Qifan and Ge, Weifeng and Huang, Lifu},
  journal={arXiv preprint arXiv:2410.03290},
  year={2024}
}

@article{grattafiori2024llama,
  title={The llama 3 herd of models},
  author={Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{lu2024deepseek,
  title={Deepseek-vl: towards real-world vision-language understanding},
  author={Lu, Haoyu and Liu, Wen and Zhang, Bo and Wang, Bingxuan and Dong, Kai and Liu, Bo and Sun, Jingxiang and Ren, Tongzheng and Li, Zhuoshu and Yang, Hao and others},
  journal={arXiv preprint arXiv:2403.05525},
  year={2024}
}

@article{chen2024expanding,
  title={Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling},
  author={Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and others},
  journal={arXiv preprint arXiv:2412.05271},
  year={2024}
}

@article{agrawal2024pixtral,
  title={Pixtral 12B},
  author={Agrawal, Pravesh and Antoniak, Szymon and Hanna, Emma Bou and Bout, Baptiste and Chaplot, Devendra and Chudnovsky, Jessica and Costa, Diogo and De Monicault, Baudouin and Garg, Saurabh and Gervet, Theophile and others},
  journal={arXiv preprint arXiv:2410.07073},
  year={2024}
}

@article{li2024aria,
  title={Aria: An open multimodal native mixture-of-experts model},
  author={Li, Dongxu and Liu, Yudong and Wu, Haoning and Wang, Yue and Shen, Zhiqi and Qu, Bowen and Niu, Xinyao and Wang, Guoyin and Chen, Bei and Li, Junnan},
  journal={arXiv preprint arXiv:2410.05993},
  year={2024}
}

@article{laurenccon2023obelics,
  title={Obelics: An open web-scale filtered dataset of interleaved image-text documents},
  author={Lauren{\c{c}}on, Hugo and Saulnier, Lucile and Tronchon, L{\'e}o and Bekman, Stas and Singh, Amanpreet and Lozhkov, Anton and Wang, Thomas and Karamcheti, Siddharth and Rush, Alexander and Kiela, Douwe and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={71683--71702},
  year={2023}
}

@article{galib2024h2ovl,
  title={H2OVL-Mississippi Vision Language Models Technical Report},
  author={Galib, Shaikat and Wang, Shanshan and Xu, Guanshuo and Pfeiffer, Pascal and Chesler, Ryan and Landry, Mark and Ambati, Sri Satish},
  journal={arXiv preprint arXiv:2410.13611},
  year={2024}
}

@article{wang2024qwen2,
  title={Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}

@article{yang2024qwen2,
  title={Qwen2. 5 technical report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@article{cheng2024videollama,
  title={Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms},
  author={Cheng, Zesen and Leng, Sicong and Zhang, Hang and Xin, Yifei and Li, Xin and Chen, Guanzheng and Zhu, Yongxin and Zhang, Wenqi and Luo, Ziyang and Zhao, Deli and others},
  journal={arXiv preprint arXiv:2406.07476},
  year={2024}
}

@article{zhang2025videollama,
  title={VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding},
  author={Zhang, Boqiang and Li, Kehan and Cheng, Zesen and Hu, Zhiqiang and Yuan, Yuqian and Chen, Guanzheng and Leng, Sicong and Jiang, Yuming and Zhang, Hang and Li, Xin and others},
  journal={arXiv preprint arXiv:2501.13106},
  year={2025}
}

@article{li2024llava,
  title={Llava-onevision: Easy visual task transfer},
  author={Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Zhang, Peiyuan and Li, Yanwei and Liu, Ziwei and others},
  journal={arXiv preprint arXiv:2408.03326},
  year={2024}
}

@misc{zhang2024llavanextvideo,
  title={LLaVA-NeXT: A Strong Zero-shot Video Understanding Model},
  url={https://llava-vl.github.io/blog/2024-04-30-llava-next-video/},
  author={Zhang, Yuanhan and Li, Bo and Liu, haotian and Lee, Yong jae and Gui, Liangke and Fu, Di and Feng, Jiashi and Liu, Ziwei and Li, Chunyuan},
  month={April},
  year={2024}
}

@inproceedings{wang2024internvideo2,
  title={Internvideo2: Scaling foundation models for multimodal video understanding},
  author={Wang, Yi and Li, Kunchang and Li, Xinhao and Yu, Jiashuo and He, Yinan and Chen, Guo and Pei, Baoqi and Zheng, Rongkun and Wang, Zun and Shi, Yansong and others},
  booktitle={European Conference on Computer Vision},
  pages={396--416},
  year={2024},
  organization={Springer}
}

@article{wang2025internvideo2,
  title={InternVideo2. 5: Empowering Video MLLMs with Long and Rich Context Modeling},
  author={Wang, Yi and Li, Xinhao and Yan, Ziang and He, Yinan and Yu, Jiashuo and Zeng, Xiangyu and Wang, Chenting and Ma, Changlian and Huang, Haian and Gao, Jianfei and others},
  journal={arXiv preprint arXiv:2501.12386},
  year={2025}
}

@techreport{gpt4o,
    author = {OpenAI},
    title = {Hello GPT-4o},
    year = {2024},
    url = {https://openai.com/index/hello-gpt-4o}
}

@inproceedings{zhou2025feature4x,
  title={Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile Gaussian Feature Fields},
  author={Zhou, Shijie and Ren, Hui and Weng, Yijia and Zhang, Shuwang and Wang, Zhen and Xu, Dejia and Fan, Zhiwen and You, Suya and Wang, Zhangyang and Guibas, Leonidas and Kadambi, Achuta},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2025}
}


@inproceedings{grauman2022ego4d,
  title={Ego4d: Around the world in 3,000 hours of egocentric video},
  author={Grauman, Kristen and Westbury, Andrew and Byrne, Eugene and Chavis, Zachary and Furnari, Antonino and Girdhar, Rohit and Hamburger, Jackson and Jiang, Hao and Liu, Miao and Liu, Xingyu and others},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={18995--19012},
  year={2022}
}



@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@inproceedings{xu2018youtube,
  title={Youtube-vos: Sequence-to-sequence video object segmentation},
  author={Xu, Ning and Yang, Linjie and Fan, Yuchen and Yang, Jianchao and Yue, Dingcheng and Liang, Yuchen and Price, Brian and Cohen, Scott and Huang, Thomas},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={585--601},
  year={2018}
}

@inproceedings{zheng2024llamafactory,
  title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models},
  author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Zhangchi Feng and Yongqiang Ma},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)},
  address={Bangkok, Thailand},
  publisher={Association for Computational Linguistics},
  year={2024},
  url={http://arxiv.org/abs/2403.13372}
}

@article{kerbl2023gaussian,
  author={Kerbl, Bernhard and Kopanas, Georgios and Leimkühler, Thomas and Drettakis, George},
  title={3D Gaussian Splatting for Real-Time Radiance Field Rendering},
  journal={ACM Transactions on Graphics (TOG)},
  volume={42},
  number={4},
  pages={1--14},
  year={2023}
}

@misc{lei2024mosca,
  author={Lei, Jiahui and Weng, Yijia and Harley, Adam and Guibas, Leonidas and Daniilidis, Kostas},
  title={MoSca: Dynamic Gaussian Fusion from Casual Videos via 4D Motion Scaffolds},
  year={2024},
  eprint={2405.17421},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}