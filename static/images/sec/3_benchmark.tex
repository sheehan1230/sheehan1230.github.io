\section{The \texttt{VLM4D} Benchmark}
We introduce \texttt{VLM4D}, the first benchmark specifically designed to test the spatiotemporal reasoning abilities of VLMs. \texttt{VLM4D} consists of 1,000 videos paired with over 2,000 question-answer pairs, each carefully designed to assess both spatial and temporal understanding jointly. The majority of these videos are sourced from datasets with rich spatiotemporal characteristics, thus ensuring a diverse range of motion-related scenarios. We augment the dataset with synthetic videos generated by a world-foundation model, Cosmos~\cite{agarwal2025cosmos}, that has been modified using techniques introduced in~\cite{he2024mojito} to obtain more accurate correspondence between motion-oriented prompts and the resulting generated video. Figure~\ref{fig:data_piechart} illustrates the composition of our dataset.

\subsection{Benchmark Construction}
Unlike prior work that often relies heavily on LLMs and VLMs to generate first iterations of benchmarks and datasets~\cite{chen2024sharegpt4video} followed by human quality control - we found that existing VLMs and automated methods showed significant limitations in terms of realiability and quality. This shortcoming necessitated direct human annotations that were then followed by augmentation by LLMs to ensure a high-quality benchmark. An overview of the benchmark curation pipeline is shown in \cref{fig:dataset_pipeline}. 

\paragraph{Real Video Data Collection} Real-world videos were sourced from datasets with rich spatiotemporal characteristics that ensured diverse motion and perspective variations. For egocentric data, we primarily relied on the Ego4D dataset~\cite{grauman2022ego4d}, while most object-centric data points were collected from the Davis~\cite{davis2017} and YouTube-VOS~\cite{xu2018youtube} datasets. To minimize confounders and to focus attention of VLM abilities to only spatiotemporal reasoning, we preprocessed the videos by temporally segmenting and centering them around the most relevant action thus resulting in videos with an average duration of $5$-$15$ seconds. This ensures that the key event described in the question is clear and reduces ambiguities or confounders that would reduce VLM accuracy. 

\paragraph{Synthetic Video Generation}  
For synthetic video generation, we use Cosmos~\cite{agarwal2025cosmos} as our video generation backbone. To ensure that the generated videos align with the intended object moving directions, we incorporate input bounding boxes as additional spatial guidance. Specifically, we follow the approach introduced in~\cite{he2024mojito} modifying the diffusion forward steps to enforce object localization constraints at each timestep, ensuring consistency between the generated object direction and the user-specified trajectory. The average duration of generated synthetic videos is $5$ seconds. To maintain high-quality outputs, we perform a manual verification step after generation, filtering out low-quality videos and retaining only those that accurately match the specified directions. Once a video is generated, we use an LLM (GPT-4o) to generate two types of questions for evaluation: Direct questions, which are derived directly from the textual prompt used to generate the video; Counterfactual questions, which involve querying about non-existent objects in the generated scene. Both question types follow the format: ``What direction is the $\langle$Object Name$\rangle$ moving?", where the model must select one of four possible answers: ``left", ``right", ``not moving", or ``no $\langle$Object Name$\rangle$ there." 


\paragraph{QA Generation and Quality Control} Question-answer pairs are primarily constructed through human annotations. The question answer pairs are then supplemented with alternative answers by an LLM (GPT-4o) for multiple choice (MC) questions. To ensure high-quality annotations, a rigorous human verification process was applied where ambiguous videos were filtered out and vague, misleading, or incorrect QA pairs were refined to allow for spatial and temporal alignment between the language and visual content. Figure~\ref{fig:qualitative_examples} showcases some qualitative examples of annotations for different types of videos.

\paragraph{Assessing Human Performance}
To establish a human performance baseline on our benchmark, we conducted an evaluation in which participants independently answered 100 randomly sampled questions from the dataset. The accuracy of human responses was then aggregated to approximate the performance of human spatiotemporal reasoning on thedataset. 

\subsection{Categorizing Spatiotemporal Performance}
To systematically evaluate spatiotemporal reasoning capabilities, we first categorize videos into two primary groups: egocentric (first-person) videos and exocentric (third-person) videos. Egocentric videos are sourced from the Ego-4D~\cite{grauman2022ego4d} dataset where scenes are captured from a head-mounted camera, thus offering dynamic video data that is inherently coupled with the individual's actions. Exocentric videos encompass a diverse range of recorded scenes, from sports footage to everyday scenes. Beyond this categorization, we also evaluate spatiotemporal performance across four dimensions: translational movement (TM), rotational movement (RM), spatiotemporal counting (STM), and false positives (FP). Translational movement assesses a model's ability to track linear motion within scenes, while rotation movement evaluates the understanding of changes in orientation and perspective shifts over time. Spatiotemporal counting extends these core motion-based tasks by requiring a more complex reasoning strategy to determine the number objects performing a translation or rotational movement. Lastly, the false positives category measures the model's reliability in recognizing whether any motion took place. By structuring the benchmark along these axes, we aim for a comprehensive framework for assessing spatiotemporal reasoning (Figure~\ref{fig:radar_plot}).
