\clearpage
\setcounter{page}{1}
\maketitlesupplementary

% \newcommand{\achuta}[1]{\textcolor{magenta}{Achuta: #1}} 

\renewcommand\thesection{\Alph{section}} % section with alphabet
\renewcommand\thesubsection{\thesection.\arabic{subsection}} % subsection with alphabet
\renewcommand\thefigure{\Alph{figure}} % figure with alphabet
\renewcommand\thetable{\Alph{table}} % table with alphabet

% Support for easy cross-referencing
% \usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\newcommand{\tabnohref}[1]{Tab.~{\color{red}#1}} % refer to table without hyper reference
\newcommand{\fignohref}[1]{Fig.~{\color{red}#1}} % refer to figure without hyper reference
\newcommand{\secnohref}[1]{Sec.~{\color{red}#1}} % refer to section without hyper reference
%\newcommand{\linenohref}[1]{L#1} % refer to line without hyper reference
\newcommand{\cnohref}[1]{[{\color{green}#1}]} % refer to citation without
\newcommand{\linenohref}[1]{Line~{\color{red}#1}}



\noindent \textbf{Appendix Outline}
\begin{itemize}[itemsep=0em]
    \item Section~\ref{sec:vlm4d_stats} Technical details about VLM4D construction and data curation 
    \item Section~\ref{sec:eval_setup} Evaluation setup 
    \item Section~\ref{sec:sft} Analysis of Existing Video Instruction Tuning Datasets
    \item Section~\ref{sec:Details of Feature4x} contains more details of 4D reconstruction using Feature4x;
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{VLM4D Stats}
\label{sec:vlm4d_stats}
\begin{table}[h]
    \centering
    \begin{tabular}{|l|c|}
        \hline
        \textbf{Dataset Statistics} & \textbf{Count} \\
        \hline
        Real Samples & 1,371 \\
        Synthetic Samples & 800 \\
        Total Samples & 2171 \\
        \hline
    \end{tabular}
    \caption{\textbf{VLM4D Dataset Breakdown}}
    \label{tab:vlm4d_stats}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Evaluation setup}
\label{sec:eval_setup}
\begin{table}[h]
    \centering
    \begin{tabular}{|l|c|c|}
        \hline
        \textbf{Task} & \textbf{GPU Configuration} \\
        \hline
        Model Evaluation & 8xA100 \\
        4D Feature Field Reconstruction & 1xA100 \\
        \hline
    \end{tabular}
    \caption{\textbf{Evaluation Type and GPU Requirements}}
    \label{tab:evaluation_setup}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Visual Instruction Tuning Dataset Analysis}
\label{sec:sft}


\begin{figure*}[t]
  \centering
  \includegraphics[width=1\textwidth]{figures/supp/all_cats.png}\vspace{-3mm}
  \caption{\textbf{Performance comparison of various VLMs across annotated
question categories including counting, rotational motion, translational motion, false positives, and action recognition}}
  \label{model_acc_vs_category}
\end{figure*}

\subsection{Analysis of Video Instruction Tuning Datasets}

We begin by analyzing four individual collections of datasets, which together contribute a substantial body of data for our experiments. The datasets used in this study are:

\begin{itemize}
    \item \textbf{ShareGPT-4o}
    \item \textbf{VideoChat2-IT}
    \item \textbf{ShareGPT4Video}
    \item \textbf{LLava-178k} 
\end{itemize}

In total, these datasets contain over 2 million samples, a robust foundation for evaluating and benchmarking the spatiotemporal validity of the highly used video instruction tuning datasets . 

In our comprehensive analysis of the ShareGPT-4o dataset ~\ref{4o}, we observed that over 40\% of the captions incorporate at least one target category. As depicted in Figure~\ref{TS}, our target string search highlights a significant emphasis on the directional descriptors "left" and "right." Furthermore, the analysis of negative samples, illustrated in Figure~\ref{ex}, indicates that these directional terms are seldom employed in conjunction with rotational or translational actions. This observation is further substantiated by the minimal overlaps between directional descriptors and action-related terms, as shown in Figure~\ref{TS}. Collectively, this shows a notable gap in the dataset's ability to capture complex spatiotemporal relationships, particularly those involving dynamic textures and nuanced motion patterns.


\begin{table}[h]
    \centering
    \renewcommand{\arraystretch}{1.2}
    \label{tab:dataset_distribution}
    \begin{tabular}{l c}
        \toprule
        \textbf{Dataset} & \textbf{Count} \\
        \midrule
        VideoChat2-IT & 423,497 \\
        ShareGPT4Video & 40,178 \\
        LLaVA-178K & 1,627,017 \\
        ShareGPT-4o & 2,111 \\
        \midrule
        \textbf{Total} & \textbf{2,092,803} \\
    \end{tabular}
     \caption{\textbf{No. of Video Instruction Tuning Samples (Q-A pair)}}
    \label{tab:dataset_sizes}
\end{table}



\begin{table}[h]
    \centering
    \begin{tabular}{|l|c|c|}
        \hline
        \textbf{Caption Category} & \textbf{Count} & \textbf{Percentage (\%)} \\
        \hline
        With Target String & 1,164,006 & 55.6 \\
        Without Target String & 928,897 & 44.4 \\
        \hline
    \end{tabular}
    \caption{\textbf{Distribution of Captions Containing Target Strings}}
    \label{tab:caption_distribution}
\end{table}




\begin{table*}[h]
    \centering
    \resizebox{1.0\linewidth}{!}{
    \renewcommand{\arraystretch}{1.2}
    % \arrayrulecolor{black} % Ensures solid borders remain black
    \begin{tabular}{|l|p{15cm}|} % Use p{width} for text wrapping in the second column
    \hline
    \textbf{Category} & \textbf{Terms} \\
    \hline
    \textbf{Directional Descriptors} & left, right, up, down, north, south, east, west, ahead, behind, towards the front, away from the, to the left, toward, to the right, in front of, behind you, side to side, straight ahead, high ground, low ground, left and right, front and back, top and bottom, northern, southern, eastern, western, northeast, northwest, southeast, southwest, around \\
    \hline
    \textbf{Translational Actions} & moving, running, walking, sprinting, gliding, sliding, crawling, trotting, jogging, skipping, bounding, rushing, hurrying, traveling, shifting, advancing, progressing, traversing, racing, zooming, going fast, going \\
    \hline
    \textbf{Rotational Descriptors} & rotate, revolve, spin, gyrate, twirl, whirl, twist, turn, pivot, flip, roll, spiral, swing, shake, oscillate, swing around, rolls, roll \\
    \hline
    \textbf{Rotational Actions} & clockwise, anticlockwise, turn right, turn left, spin, rotate, revolve, twist, pivot, gyrate, whirl, rotating, spinning, turning 360 degrees, turning 180 degrees,  rotation, twisting, turning around, circular motion, turning 90 degrees, \\
    \hline
    \textbf{Perspective Descriptors} & camera’s perspective, frame’s perspective, viewpoint, point of view, line of sight, from above, from below, from the side, from the front, from behind, top view, bottom view, rear view, side view, front view, bird’s eye view, aerial view, close-up view, distant view \\
    \hline
    \end{tabular}
    }
    \caption{\textbf{Video Instruction Tuning Datasets Categories \& Target strings Analysis}}
\label{tab:target_categories}
\end{table*}


\begin{figure*}[t]
  \centering
  \includegraphics{figures/supp/Llava-178k.pdf}\vspace{-3mm}
  \caption{\textbf{Heatmap of Occurrences of Spatial-Temporal Terms in LLava-178k}}
  
    {\small \textit{(Note: LLava-178k actually comprises over 1.6 million samples, as we combine many of the available dataset splits within this collection.)}}
    
  \label{Q&A_example1}
\end{figure*}

\begin{figure*}[t]
  \centering
  \includegraphics{figures/supp/ShareGPT4Video.pdf}\vspace{-3mm}
  \caption{\textbf{Heatmap of Occurrences of Spatial-Temporal Terms in ShareGPT4Video}}
  \label{Q&A_example2}
\end{figure*}

\begin{figure*}[t]
  \centering
  \includegraphics{figures/supp/vidchat2.pdf}\vspace{-3mm}
  \caption{\textbf{Heatmap of Occurrences of Spatial-Temporal Terms in VideoChat-IT}}
  \label{Q&A_example3}
\end{figure*}

\begin{figure*}[t]
  \centering
  \includegraphics{figures/supp/ShareGPT4o.pdf}\vspace{-3mm}
  \caption{\textbf{Heatmap of Occurrences of Spatial-Temporal Terms in ShareGPT-4o}}
  \label{4o}
\end{figure*}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/supp/ShareGPT-4o_example.pdf}\hspace{8mm}  % Reduce width and add right padding
    \caption{{\textit{Example of a clip with multiple target categories. Spatiotemporal grounding remains a challenge, as the direction descriptor is \textbf{incorrect}, and the translational actions provide limited insight, primarily indicating that objects/subjects are not static.}}}
  \label{ex}
\end{figure*}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/supp/4o_TS.pdf}\hspace{8mm}  % Reduce width and add right padding
    \caption{\textbf{Heatmap to visualize the prevalence of co-occurring target strings across the ShareGPT-4o dataset, informing our evaluation of spatiotemporal grounding in video instruction data.}}
  \label{TS}
\end{figure*}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Details of 4D Reconstruction}
\label{sec:Details of Feature4x}
We utilize the Feature4X framework~\cite{zhou2025feature4x} for 4D reconstruction experiments conducted on our dataset. Given an input monocular RGB video, Feature4X reconstructs the dynamic 3D scene by employing dynamic 3D Gaussians, specifically Dynamic 3D Gaussian Splatting~\cite{kerbl2023gaussian, lei2024mosca}, which represent dynamic foreground elements that deform over time. These dynamic Gaussians are guided by a 4D Motion Scaffold, a sparse graph of trajectory nodes, enabling the interpolation of dense motion trajectories and features for each Gaussian efficiently. A separate set of static 3D Gaussians represents static background elements.

Feature4X introduces a unified latent feature embedding, distilled from various foundational 2D models, which facilitates multiple downstream tasks such as segmentation, scene editing, and visual question answering (VQA). Specifically, Feature4X extracts video segment features from the InternVideo2-Chat model~\cite{wang2024internvideo2,li2023videochat}, a foundation model fine-tuned for video question answering. 

This unified feature field is directly used by the InternVideo decoder for inference, bypassing the video encoding step entirely. This approach significantly improves inference efficiency and retains comprehensive structural information from the 4D scene representation, which surpasses the context available from the original 2D videos alone. Consequently, this method enhances downstream tasks by providing richer spatiotemporal context and semantic consistency.





\begin{figure*}[t]
  \centering
  \includegraphics[width=1\textwidth]{figures/supp/VLM_4DRecon.pdf}\vspace{-3mm}
  \label{fig:4D_recon}
  \caption{\textbf{General Input-Output Architecture of Feature4X} }
\end{figure*}


\begin{figure*}[t]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/supp/Q&A_Split1.pdf}\vspace{-3mm}
  \label{fig:Q&A_example}
  \caption{\textbf{Complete CoT and DO Responses of VLMs} Models 1-3}
\end{figure*}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/supp/Q&A_Split2.pdf}\vspace{-3mm}
  \caption{Models 4-8}
  \label{fig:Q&A_example}
\end{figure*}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/supp/Q&A_Split3.pdf}\vspace{-3mm}
  \caption{Models 9-12}
  \label{fig:Q&A_example}
\end{figure*}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/supp/Q&A_Split4.pdf}\vspace{-3mm}
  \caption{Models 13-17}
  \label{fig:Q&A_example}
\end{figure*}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/supp/Q&A_Split5.pdf}\vspace{-3mm}
  \caption{Models 18-27}
  \label{fig:Q&A_example}
\end{figure*}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/supp/Q&A_Split6.pdf}\vspace{-3mm}
  \caption{Model 28}
  \label{fig:Q&A_example}
\end{figure*}



% \clearpage
% %%%%%%%% REFERENCESsa
% 
% \small
% \bibliographystyle{ieee_fullname}
% \bibliography{supplement}
% }

% \clearpage
% {
% \small
% \bibliographystyle{ieeenat_fullname_sup}
% \bibliography{supplement}
% }

% \end{document}